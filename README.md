# Hierarchical Semantic RL (HSRL)

Official implementation of the paper:<br>
**‚ÄúHierarchical Semantic RL: Tackling the Problem of Dynamic Action Spaces for RL-based Recommendation.‚Äù**

---

## üöÄ Overview

**HSRL** is a novel reinforcement learning framework for recommender systems that addresses one of the most critical bottlenecks in RL-based recommendation ‚Äî the **dynamic and high-dimensional action space**.

HSRL introduces a **Semantic Action Space (SAS)**, where each item is represented by a compact, hierarchical **Semantic Identifier (SID)**.
Through this design, HSRL achieves **structured decision-making**, **stable training**, and **interpretable long-term optimization**.

---

## üåü Key Features

| Category            | Description                                                                                                                                                                                                                                                                                |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Problem**         | RL-based recommenders face **huge and constantly changing action spaces**, making policy learning unstable and inefficient.                                                                                                                                                                |
| **Solution**        | **Hierarchical Semantic Reinforcement Learning (HSRL)** defines a fixed **Semantic Action Space (SAS)** to decouple policy learning from item dynamics.                                                                                                                                    |
| **Core Components** | ‚Ä¢ **Semantic Identifiers (SIDs):** Compact, invertible item representations.<br>‚Ä¢ **Hierarchical Policy Network (HPN):** Coarse-to-fine token generation with residual state modeling.<br>‚Ä¢ **Multi-Level Critic (MLC):** Token-level value estimation for fine-grained credit assignment. |
| **Results**         | ‚Ä¢ Consistently outperforms SOTA methods on public and industrial datasets.<br>‚Ä¢ **7-day online A/B test:** +18.421% CVR gain with only +1.251% cost increase.                                                                                                                              |
| **Impact**          | Demonstrates **semantic action modeling** as a **scalable and interpretable paradigm** for large-scale RL-based recommendation.                                                                                                                                                            |

---



## ‚öôÔ∏è Setup

### 0. Pretrain the User Response Model (Environment Component)

Modify `train_env.sh`:

* Update dataset paths:

  ```bash
  data_path=/your/path/to/dataset/
  output_path=/your/path/to/output/
  ```
* Configure model arguments (`X ‚àà {RL4RS, ML1M}`):

  ```bash
  --model {X}UserResponse \
  --reader {X}DataReader \
  --train_file ${data_path}{X}_b_train.csv \
  --val_file ${data_path}{X}_b_test.csv
  ```
* Set `model_path` and `log_path`.

Run:

```bash
bash train_env.sh
```

---

### 1. Build the Semantic Codebook

```bash
cd dataset
python build_codebook.py
python build_item2sid.py
```

---

### 2. Training

#### 2.1 Available Scripts

| Task                        | Script                    |
| --------------------------- | ------------------------- |
| DDPG                        | `bash train_ddpg.sh`      |
| BehaviorDDPG                | `bash train_superddpg.sh` |
| Offline Supervised Learning | `bash train_supervise.sh` |
| HSRL                        | `bash train_sid_rl4rs.sh` |

#### 2.2 Continue Training

To resume from a checkpoint:

```bash
--n_iter ${PREVIOUS_N_ITER} ${N_ITER}
```

---

### 3. Evaluation & Analysis

Run testing:

```bash
bash test.sh
```

Visualize and analyze results:

```bash
result_analysis.ipynb
```


## üß© Acknowledgements

This project builds upon the **RL4RS** framework and extends it with **hierarchical semantic representations** for structured reinforcement learning in large-scale recommendation scenarios.